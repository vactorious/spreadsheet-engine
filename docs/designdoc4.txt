CS130 Project 4 - Design Document
=================================

Please answer all questions in this design document.  Note that the final
feedback section is optional, and you are not required to answer it if you
don't want to.

Unanswered or incompletely answered questions, or answers that don't actually
match the code/repository, will result in deductions.

Answers don't have to be deeply detailed!  We are mainly looking for an
overview or summary description of how your project works, and your team's
experiences working on this project.

Logistics (7 pts)
-----------------

L1.  [2pts] Enumerate all teammates here.
          Victor Huang, Matthew Gonzalgo, John Kim

L2.  [2pts] What did each teammate focus on during this project?
          Victor: performance analysis, Lark grammar changes, functions
          Matthew: Implementing conditional expressions and function calls
          John: Implementing conditional expressions and function calls

L3.  [3pts] Approximately how many hours did each teammate spend on the project?
          ~ 10 hours each

Spreadsheet Engine Design (27 pts)
----------------------------------

D1.  [4pts] Briefly describe the changes you made to the Lark parser grammar to
     support Boolean literals and conditional expressions.  How did you ensure
     that conditional expressions are lower precedence than arithmetic and
     string concatenation operations?
          We added support for boolean literals and conditional expressions by
          creating an expression for comparisons (compare_expr), a boolean
          base value, and compare operators. We make sure that conditional
          expressions are lower precedence than the others by assigning
          "priority" values to each type in the transformer class (not in
          the parser grammar). If the two arguments aren't of the same type,
          then the priority values are compared instead.

D2.  [5pts] Briefly describe how function invocation works in your spreadsheet
     engine.  How easy or hard would it be for you to add new functions to your
     engine?  What about a third-party developer?  How well does your code
     follow the Open/Closed Principle?
          The lark parser grammar is set up to recognize the format of
          function calls. We define a dictionary in functions.py mapping
          funciton names to the function callables, and use this to call the
          functions directly from the transformer. Adding a new function is
          easy and only requires the dev to add a new key-value pair in the
          dictionary and define the function. Our code follows the Open/Closed
          Principle for the most part, except for devs having to modify the
          dictionary (which counts as original source code) in order for the
          transformer to recognize a new function.

D3.  [5pts] Is your implementation able to lazily evaluate the arguments to
     functions like IF(), CHOOSE() and IFERROR()?  (Recall from the Project 4
     spec that your spreadsheet engine should not report cycles in cases where
     an argument to these functions does not need to be evaluated.)  If so,
     what changes to your design were required to achieve this?  If not, what
     prevented your team from implementing this?
          No, our implementation does not currently lazily evaluate arguments.
          This is because we use a transformer to evaluate formulas, so all
          expressions are evaluated bottom-up. Thus, all of the subtrees
          are evaluated first before we reach the function calls.

D4.  [5pts] Is your implementation able to evaluate the ISERROR() function
     correctly, with respect to circular-reference errors?  (Recall from the
     Project 4 spec that ISERROR() behaves differently when part of a cycle,
     vs. being outside the cycle and referencing some cell in the cycle.)
     If so, what changes to your design were required to achieve this?  If
     not, what prevented your team from implementing this?
          Our implementation is able to evaluate the ISERROR() function
          correctly with respect to circular-reference errors. Since Project 1,
          we have implemented Kosaraju's algorithm and use it in our update
          process to detect strongly connected components. This plays a
          critical role in the ISERROR() function.

D5.  [5pts] Is your implementation able to successfully identify cycles that
     are not evident from static analysis of formulas containing INDIRECT()?
     If so, what changes to your design were required, if any, to achieve this?
     If not, what prevented your team from implementing this?
          Our implementation is not able to successfully identify cycles that
          are not evident from static analysis of formulas containing
          INDIRECT(). This is because we were not able to convert our
          EvalExpressions Transformer into an Interpreter.

D6.  [3pts] Project 4 has a number of small but important operations to
     implement:  Comparison operations include a number of comparison and type
     conversion rules.  Different functions may require specific numbers and
     types of arguments.  How did your team structure the implementation of
     these operations?  How did your approach affect the reusability and
     testability of these operations?
          The comparison and type conversion rules were implemented simply
          using conditional statements in the transformer. While this was
          simple to implement, changing these rules in the future might be less
          reusable and more cumbersome. The number of arguments were enforced
          within each function: each takes in a list of arguments (could be
          empty), and if the length of the list or the types are not correct,
          then an error is returned.

Performance Analysis (16 pts)
-----------------------------

In this project you must measure and analyze the performance of features that
generate large bulk changes to a workbook:  loading a workbook, copying or
renaming a sheet, and moving or copying an area of cells.  Construct some
performance tests to exercise these aspects of your engine, and use a profiler
to identify where your program is spending the bulk of its time.

A1.  [4pts] Briefly enumerate the performance tests you created to exercise
     your implementation.
          We implemented five different performance tests for this project.
          First, we created a json file of a massive workbook, which would
          be used for the tests. The workbook consisted of four sheets, each
          with a 500 x 500 block of cells. The first test loaded the json file,
          and stores it in a class attribute for use in other tests (so we
          don't repeat the loading time in the other tests). The second test
          copies one of the sheets in the created workbook. The third test
          renames one of the sheets in the workbook. The fourth test moves one
          of the cell blocks in a sheet to another part of the sheet. The fifth
          test does the same thing as the fourth one, but uses copy.

A2.  [4pts] What profiler did you choose to run your performance tests with?
     Why?  Give an example of how to invoke one of your tests with the profiler.
          We chose to use cProfile because it is a built-in python module that
          can provide all of the profiling functionality that we need. We
          created a command in the makefile to start all performance tests:
          make test-performance. The stress testing class has a setUp()
          function which starts up the profiler. It also has a tearDown()
          function which prepares results for printing (sorting, cleaning). A
          list of function calls and timings are produced for each performance
          test. This way, by typing make test-performance, each test case will
          output the functions called and time taken for each call.

A3.  [8pts] What are ~3 of the most significant hot-spots you identified in your
     performance testing?  Did you expect these hot-spots, or were they
     surprising to you?
          1. One of the functions that frequently popped up in the tests was
             quantify_cell_loc, which converts a cell location ("A1") and
             returns a column and row integer tuple. This hot spot is expected
             because the function is called many times, including in formula
             evaluation as well as set_cell_contents.
          2. Another hot-spot that popped up frequently was the cycle-detection
             algorithm. This was kind of unexpected because the maximum cell
             reference depth in the testing workbook is 1 cell deep, so the
             chain lengths are at most 2.
          3. The third hot-spot was the cell_from_loc function, which returns
             a cell instance given a location in a sheet. This was expected
             because the way we store cells requires usage of quantify_cell_loc
             to access the data structure, so revamping that data structure
             should improve things.


Section F:  CS130 Project 3 Feedback [OPTIONAL]
-----------------------------------------------

These questions are OPTIONAL, and you do not need to answer them.  Your grade
will not be affected by answering or not answering them.  Also, your grade will
not be affected by negative feedback - we want to know what went poorly so that
we can improve future versions of the course.

F1.  What parts of the assignment did you find highly enjoyable?  Conversely,
     what parts of the assignment did you find unenjoyable?


F2.  What parts of the assignment helped you learn more about software
     engineering best-practices, or other useful development skills?
     What parts were not helpful in learning these skills?


F3.  Were there any parts of the assignment that seemed _unnecessarily_ tedious?
     (Some parts of software development are always tedious, of course.)


F4.  Do you have any feedback and/or constructive criticism about how this
     project can be made better in future iterations of CS130?